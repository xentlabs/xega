import os
from typing import List

import click

from xega.benchmark.expand_benchmark import expand_benchmark_config
from xega.cli.cli_util import generate_benchmark_id
from xega.common.util import dumps
from xega.common.xega_types import (
    GameConfig,
    PlayerConfig,
    XegaBenchmarkConfig,
    XegaMetadata,
)
from xega.runtime.llm_api_client import guess_provider_from_model

SIMPLE_GAME_CODE = """
assign(s=story())
reveal(black, s)
elicit(black, x, 10)
assign(x1=remove_common_words(x, s)) # Remove any words in story from input text
reward(black, xed(s | x1))
""".strip()


DEFAULT_XEGA_CONFIG = XegaMetadata(
    judge_model="gpt2",
    auto_replay=True,
    max_steps=100,
    seed="notrandom",
    num_variables_per_register=4,
    npc_players=[],
    num_maps_per_game=1,
)


def games_from_dir(game_dir: str) -> List[GameConfig]:
    files = os.listdir(game_dir)
    game_configs = []
    for file_name in files:
        if not file_name.endswith(".xega"):
            continue
        game_name = file_name[:-5]
        with open(os.path.join(game_dir, file_name), "r") as f:
            game_code = f.read()
            game_config = GameConfig(
                name=game_name,
                code=game_code,
            )
            game_configs.append(game_config)
    return game_configs


def build_benchmark_config(
    models: List[str],
    judge: str,
    games: List[GameConfig],
    benchmark_id: str,
    seed: str,
    num_steps: int,
    auto_replay: bool,
    num_maps_per_game: int,
):

    players = [
        [
            PlayerConfig(
                name="black",
                id=model,
                player_type="default",
                options={"model": model, "provider": guess_provider_from_model(model)},
            )
        ]
        for model in models
    ]
    return XegaBenchmarkConfig(
        config_type="short_benchmark_config",
        games=games,
        players=players,
        benchmark_id=benchmark_id,
        judge_model=judge,
        npc_players=DEFAULT_XEGA_CONFIG["npc_players"],
        max_steps=num_steps,
        auto_replay=auto_replay,
        num_variables_per_register=DEFAULT_XEGA_CONFIG["num_variables_per_register"],
        seed=seed,
        num_maps_per_game=num_maps_per_game,
    )


@click.command()
@click.option(
    "--output", help="Output configuration path", default="./xega_config.json"
)
@click.option(
    "--game-dir",
    help='Path to directory containing games in files ending with ".xega"',
)
@click.option(
    "--model",
    multiple=True,
    default=["gpt-4o"],
    help="Add a model as a player (can be used multiple times)",
)
@click.option(
    "--judge",
    default=DEFAULT_XEGA_CONFIG["judge_model"],
    help="Specify the judge model to use for the benchmark. Default is 'gpt2'",
)
@click.option(
    "--benchmark-id",
    default=None,
    help="Specify benchmark id for configuration. A unique id will be generated by default if not specified",
)
@click.option(
    "--num-steps",
    default=DEFAULT_XEGA_CONFIG["max_steps"],
    help="Specify the maximum number of steps for the benchmark. Default is 100",
)
@click.option(
    "--no-auto-replay",
    is_flag=True,
    help="Disable auto-replay for games in the benchmark. Default is enabled. Usually you would only disable this for testing purposes",
)
@click.option(
    "--seed",
    default=DEFAULT_XEGA_CONFIG["seed"],
    help="Specify a seed for benchmark randomization. 'notrandom' is the default seed if not specified",
)
@click.option(
    "--num-maps-per-game",
    default=DEFAULT_XEGA_CONFIG["num_maps_per_game"],
    help="Specify the number of maps per game. Default is 1.",
    type=int,
)
@click.option(
    "--expand-config",
    is_flag=True,
    help="Fully expand the benchmark. This will generate a much more verbose configuration. This is useful for generating reproducible benchmarks, but the output will be much larger and more difficult to modify manually",
)
@click.option(
    "--print-config",
    is_flag=True,
    help="Print the configuration to stdout instead of writing to a file",
)
def configure(
    output: str,
    game_dir: str,
    model: List[str],
    judge: str,
    benchmark_id: str | None,
    num_steps: int,
    no_auto_replay: bool,
    seed: str,
    num_maps_per_game: int,
    print_config: bool,
    expand_config: bool,
):
    """Build Xega benchmark configuration"""
    if benchmark_id is None:
        benchmark_id = generate_benchmark_id()

    if not game_dir:
        games = [GameConfig(name="simple_game", code=SIMPLE_GAME_CODE)]
    else:
        games = games_from_dir(game_dir)

    config = build_benchmark_config(
        model,
        judge,
        games,
        benchmark_id,
        seed,
        num_steps,
        not no_auto_replay,
        num_maps_per_game,
    )

    if expand_config:
        config = expand_benchmark_config(config)

    config_str = dumps(config, indent=2)
    if print_config:
        print(config_str)
    else:
        with open(output, "w") as f:
            f.write(config_str)
            print(f"Config written to {output}")
